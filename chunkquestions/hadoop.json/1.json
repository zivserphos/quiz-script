[{"query":" Q1. Partitioner controls the partitioning of what data?","options":[" final keys\r\n"," final values\r\n"," intermediate keys\r\n"," intermediate values\r\n\r\n"],"correctAns":3},{"query":" Q2. SQL Windowing functions are implemented in Hive using which keywords?","options":[" UNION DISTINCT, RANK\r\n"," OVER, RANK\r\n"," OVER, EXCEPT\r\n"," UNION DISTINCT, RANK\r\n\r\n"],"correctAns":2},{"query":" Q3. Rather than adding a Secondary Sort to a slow Reduce job, it is Hadoop best practice to perform which optimization?","options":[" Add a partitioned shuffle to the Map job.\r\n"," Add a partitioned shuffle to the Reduce job.\r\n"," Break the Reduce job into multiple, chained Reduce jobs.\r\n"," Break the Reduce job into multiple, chained Map jobs.\r\n\r\n"],"correctAns":2},{"query":" Q4. Hadoop Auth enforces authentication on protected resources. Once authentication has been established, it sets what type of authenticating cookie?","options":[" encrypted HTTP\r\n"," unsigned HTTP\r\n"," compressed HTTP\r\n"," signed HTTP\r\n\r\n"],"correctAns":4},{"query":" Q5. MapReduce jobs can be written in which language?","options":[" Java or Python\r\n"," SQL only\r\n"," SQL or Java\r\n"," Python or SQL\r\n\r\n"],"correctAns":1},{"query":" Q6. To perform local aggregation of the intermediate outputs, MapReduce users can optionally specify which object?","options":[" Reducer\r\n"," Combiner\r\n"," Mapper\r\n"," Counter\r\n\r\n"],"correctAns":2},{"query":" Q7. To verify job status, look for the value `___` in the `___`.","options":[" SUCCEEDED; syslog\r\n"," SUCCEEDED; stdout\r\n"," DONE; syslog\r\n"," DONE; stdout\r\n\r\n"],"correctAns":2},{"query":" Q8. Which line of code implements a Reducer method in MapReduce 2.0?","options":[" public void reduce(Text key, Iterator<IntWritable> values, Context context){…}\r\n"," public static void reduce(Text key, IntWritable[] values, Context context){…}\r\n"," public static void reduce(Text key, Iterator<IntWritable> values, Context context){…}\r\n"," public void reduce(Text key, IntWritable[] values, Context context){…}\r\n\r\n"],"correctAns":1},{"query":" Q9. To get the total number of mapped input records in a map job task, you should review the value of which counter?","options":[" FileInputFormatCounter\r\n"," FileSystemCounter\r\n"," JobCounter\r\n"," TaskCounter (NOT SURE)\r\n\r\n"],"correctAns":4},{"query":" Q10. Hadoop Core supports which CAP capabilities?","options":[" A, P\r\n"," C, A\r\n"," C, P\r\n"," C, A, P\r\n\r\n"],"correctAns":1},{"query":" Q11. What are the primary phases of a Reducer?","options":[" combine, map, and reduce\r\n"," shuffle, sort, and reduce\r\n"," reduce, sort, and combine\r\n"," map, sort, and combine\r\n\r\n"],"correctAns":2},{"query":" Q12. To set up Hadoop workflow with synchronization of data between jobs that process tasks both on disk and in memory, use the `___` service, which is `___`.","options":[" Oozie; open source\r\n"," Oozie; commercial software\r\n"," Zookeeper; commercial software\r\n"," Zookeeper; open source\r\n\r\n"],"correctAns":4},{"query":" Q13. For high availability, use multiple nodes of which type?","options":[" data\r\n"," name\r\n"," memory\r\n"," worker\r\n\r\n"],"correctAns":2},{"query":" Q14. DataNode supports which type of drives?","options":[" hot swappable\r\n"," cold swappable\r\n"," warm swappable\r\n"," non-swappable\r\n\r\n"],"correctAns":1},{"query":" Q15. Which method is used to implement Spark jobs?","options":[" on disk of all workers\r\n"," on disk of the master node\r\n"," in memory of the master node\r\n"," in memory of all workers\r\n\r\n"],"correctAns":4},{"query":" Q16. In a MapReduce job, where does the map() function run?","options":[" on the reducer nodes of the cluster\r\n"," on the data nodes of the cluster (NOT SURE)\r\n"," on the master node of the cluster\r\n"," on every node of the cluster\r\n\r\n"],"correctAns":2},{"query":" Q17. To reference a master file for lookups during Mapping, what type of cache should be used?","options":[" distributed cache\r\n"," local cache\r\n"," partitioned cache\r\n"," cluster cache\r\n\r\n"],"correctAns":1},{"query":" Q18. Skip bad records provides an option where a certain set of bad input records can be skipped when processing what type of data?","options":[" cache inputs\r\n"," reducer inputs\r\n"," intermediate values\r\n"," map inputs\r\n\r\n"],"correctAns":4},{"query":" Q19. Which command imports data to Hadoop from a MySQL database?","options":[" spark import --connect jdbc:mysql://mysql.example.com/spark --username spark --warehouse-dir user/hue/oozie/deployments/spark\r\n"," sqoop import --connect jdbc:mysql://mysql.example.com/sqoop --username sqoop --warehouse-dir user/hue/oozie/deployments/sqoop\r\n"," sqoop import --connect jdbc:mysql://mysql.example.com/sqoop --username sqoop --password sqoop --warehouse-dir user/hue/oozie/deployments/sqoop\r\n"," spark import --connect jdbc:mysql://mysql.example.com/spark --username spark --password spark --warehouse-dir user/hue/oozie/deployments/spark\r\n\r\n"],"correctAns":3},{"query":" Q20. In what form is Reducer output presented?","options":[" compressed (NOT SURE)\r\n"," sorted\r\n"," not sorted\r\n"," encrypted\r\n\r\n"],"correctAns":1},{"query":" Q21. Which library should be used to unit test MapReduce code?","options":[" JUnit\r\n"," XUnit\r\n"," MRUnit\r\n"," HadoopUnit\r\n\r\n"],"correctAns":3},{"query":" Q22. If you started the NameNode, then which kind of user must you be?","options":[" hadoop-user\r\n"," super-user\r\n"," node-user\r\n"," admin-user\r\n\r\n"],"correctAns":2},{"query":" Q23. State \\_ between the JVMs in a MapReduce job","options":[" can be configured to be shared\r\n"," is partially shared\r\n"," is shared\r\n"," is not shared (https://www.lynda.com/Hadoop-tutorials/Understanding-Java-virtual-machines-JVMs/191942/369545-4.html)\r\n\r\n"],"correctAns":4},{"query":" Q24. To create a MapReduce job, what should be coded first?","options":[" a static job() method\r\n"," a Job class and instance (NOT SURE)\r\n"," a job() method\r\n"," a static Job class\r\n\r\n"],"correctAns":2},{"query":" Q25. To connect Hadoop to AWS S3, which client should you use?","options":[" S3A\r\n"," S3N\r\n"," S3\r\n"," the EMR S3\r\n\r\n"],"correctAns":1}]