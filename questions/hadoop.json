[{"query":"Partitioner controls the partitioning of what data?","options":["final keys","final values","intermediate keys","intermediate values"],"correctAns":3,"difficulty":null,"optionsAsCode":false},{"query":"SQL Windowing functions are implemented in Hive using which keywords?","options":["UNION DISTINCT, RANK","OVER, RANK","OVER, EXCEPT","UNION DISTINCT, RANK"],"correctAns":2,"difficulty":null,"optionsAsCode":false},{"query":"Rather than adding a Secondary Sort to a slow Reduce job, it is Hadoop best practice to perform which optimization?","options":["Add a partitioned shuffle to the Map job.","Add a partitioned shuffle to the Reduce job.","Break the Reduce job into multiple, chained Reduce jobs.","Break the Reduce job into multiple, chained Map jobs."],"correctAns":2,"difficulty":null,"optionsAsCode":false},{"query":"Hadoop Auth enforces authentication on protected resources. Once authentication has been established, it sets what type of authenticating cookie?","options":["encrypted HTTP","unsigned HTTP","compressed HTTP","signed HTTP"],"correctAns":4,"difficulty":null,"optionsAsCode":false},{"query":"MapReduce jobs can be written in which language?","options":["Java or Python","SQL only","SQL or Java","Python or SQL"],"correctAns":1,"difficulty":null,"optionsAsCode":false},{"query":"To perform local aggregation of the intermediate outputs, MapReduce users can optionally specify which object?","options":["Reducer","Combiner","Mapper","Counter"],"correctAns":2,"difficulty":null,"optionsAsCode":false},{"query":"To verify job status, look for the value `___` in the `___`.","options":["SUCCEEDED; syslog","SUCCEEDED; stdout","DONE; syslog","DONE; stdout"],"correctAns":2,"difficulty":null,"optionsAsCode":false},{"query":"Which line of code implements a Reducer method in MapReduce 2.0?","options":["public void reduce(Text key, Iterator<IntWritable> values, Context context){…}","public static void reduce(Text key, IntWritable[] values, Context context){…}","public static void reduce(Text key, Iterator<IntWritable> values, Context context){…}","public void reduce(Text key, IntWritable[] values, Context context){…}"],"correctAns":1,"difficulty":null,"optionsAsCode":false},{"query":"To get the total number of mapped input records in a map job task, you should review the value of which counter?","options":["FileInputFormatCounter","FileSystemCounter","JobCounter","TaskCounter (NOT SURE)"],"correctAns":4,"difficulty":null,"optionsAsCode":false},{"query":"Hadoop Core supports which CAP capabilities?","options":["A, P","C, A","C, P","C, A, P"],"correctAns":1,"difficulty":null,"optionsAsCode":false},{"query":"What are the primary phases of a Reducer?","options":["combine, map, and reduce","shuffle, sort, and reduce","reduce, sort, and combine","map, sort, and combine"],"correctAns":2,"difficulty":null,"optionsAsCode":false},{"query":"To set up Hadoop workflow with synchronization of data between jobs that process tasks both on disk and in memory, use the `___` service, which is `___`.","options":["Oozie; open source","Oozie; commercial software","Zookeeper; commercial software","Zookeeper; open source"],"correctAns":4,"difficulty":null,"optionsAsCode":false},{"query":"For high availability, use multiple nodes of which type?","options":["data","name","memory","worker"],"correctAns":2,"difficulty":null,"optionsAsCode":false},{"query":"DataNode supports which type of drives?","options":["hot swappable","cold swappable","warm swappable","non-swappable"],"correctAns":1,"difficulty":null,"optionsAsCode":false},{"query":"Which method is used to implement Spark jobs?","options":["on disk of all workers","on disk of the master node","in memory of the master node","in memory of all workers"],"correctAns":4,"difficulty":null,"optionsAsCode":false},{"query":"In a MapReduce job, where does the map() function run?","options":["on the reducer nodes of the cluster","on the data nodes of the cluster (NOT SURE)","on the master node of the cluster","on every node of the cluster"],"correctAns":2,"difficulty":null,"optionsAsCode":false},{"query":"To reference a master file for lookups during Mapping, what type of cache should be used?","options":["distributed cache","local cache","partitioned cache","cluster cache"],"correctAns":1,"difficulty":null,"optionsAsCode":false},{"query":"Skip bad records provides an option where a certain set of bad input records can be skipped when processing what type of data?","options":["cache inputs","reducer inputs","intermediate values","map inputs"],"correctAns":4,"difficulty":null,"optionsAsCode":false},{"query":"Which command imports data to Hadoop from a MySQL database?","options":["spark import --connect jdbc:mysql://mysql.example.com/spark --username spark --warehouse-dir user/hue/oozie/deployments/spark","sqoop import --connect jdbc:mysql://mysql.example.com/sqoop --username sqoop --warehouse-dir user/hue/oozie/deployments/sqoop","sqoop import --connect jdbc:mysql://mysql.example.com/sqoop --username sqoop --password sqoop --warehouse-dir user/hue/oozie/deployments/sqoop","spark import --connect jdbc:mysql://mysql.example.com/spark --username spark --password spark --warehouse-dir user/hue/oozie/deployments/spark"],"correctAns":3,"difficulty":null,"optionsAsCode":false},{"query":"In what form is Reducer output presented?","options":["compressed (NOT SURE)","sorted","not sorted","encrypted"],"correctAns":1,"difficulty":null,"optionsAsCode":false},{"query":"Which library should be used to unit test MapReduce code?","options":["JUnit","XUnit","MRUnit","HadoopUnit"],"correctAns":3,"difficulty":null,"optionsAsCode":false},{"query":"If you started the NameNode, then which kind of user must you be?","options":["hadoop-user","super-user","node-user","admin-user"],"correctAns":2,"difficulty":null,"optionsAsCode":false},{"query":"State \\_ between the JVMs in a MapReduce job","options":["can be configured to be shared","is partially shared","is shared","is not shared"],"correctAns":4,"difficulty":null,"optionsAsCode":false},{"query":"To create a MapReduce job, what should be coded first?","options":["a static job() method","a Job class and instance (NOT SURE)","a job() method","a static Job class"],"correctAns":2,"difficulty":null,"optionsAsCode":false},{"query":"To connect Hadoop to AWS S3, which client should you use?","options":["S3A","S3N","S3","the EMR S3"],"correctAns":1,"difficulty":null,"optionsAsCode":false},{"query":"HBase works with which type of schema enforcement?","options":["schema on write","no schema","external schema","schema on read"],"correctAns":4,"difficulty":null,"optionsAsCode":false},{"query":"HDFS file are of what type?","options":["read-write","read-only","write-only","append-only"],"correctAns":4,"difficulty":null,"optionsAsCode":false},{"query":"A distributed cache file path can originate from what location?","options":["hdfs or top","http","hdfs or http","hdfs"],"correctAns":3,"difficulty":null,"optionsAsCode":false},{"query":"Which library should you use to perform ETL-type MapReduce jobs?","options":["Hive","Pig","Impala","Mahout"],"correctAns":2,"difficulty":null,"optionsAsCode":false},{"query":"What is the output of the Reducer?","options":["a relational table","an update to the input file","a single, combined list","a set of <key, value> pairs"],"correctAns":4,"difficulty":null,"optionsAsCode":false},{"query":"When implemented on a public cloud, with what does Hadoop processing interact?","options":["files in object storage","graph data in graph databases","relational data in managed RDBMS systems","JSON data in NoSQL databases"],"correctAns":1,"difficulty":null,"optionsAsCode":false},{"query":"In the Hadoop system, what administrative mode is used for maintenance?","options":["data mode","safe mode","single-user mode","pseudo-distributed mode"],"correctAns":2,"difficulty":null,"optionsAsCode":false},{"query":"In what format does RecordWriter write an output file?","options":["<key, value> pairs","keys","values","<value, key> pairs"],"correctAns":1,"difficulty":null,"optionsAsCode":false},{"query":"To what does the Mapper map input key/value pairs?","options":["an average of keys for values","a sum of keys for values","a set of intermediate key/value pairs","a set of final key/value pairs"],"correctAns":3,"difficulty":null,"optionsAsCode":false},{"query":"Which Hive query returns the first 1,000 values?","options":["SELECT…WHERE value = 1000","SELECT … LIMIT 1000","SELECT TOP 1000 …","SELECT MAX 1000…"],"correctAns":2,"difficulty":null,"optionsAsCode":false},{"query":"To implement high availability, how many instances of the master node should you configure?","options":["one","zero","shared","two or more"],"correctAns":4,"difficulty":null,"optionsAsCode":false},{"query":"Hadoop 2.x and later implement which service as the resource coordinator?","options":["kubernetes","JobManager","JobTracker","YARN"],"correctAns":4,"difficulty":null,"optionsAsCode":false},{"query":"In MapReduce, **\\_** have \\_","options":["tasks; jobs","jobs; activities","jobs; tasks","activities; tasks"],"correctAns":3,"difficulty":null,"optionsAsCode":false},{"query":"What type of software is Hadoop Common?","options":["database","distributed computing framework","operating system","productivity tool"],"correctAns":2,"difficulty":null,"optionsAsCode":false},{"query":"If no reduction is desired, you should set the numbers of \\_ tasks to zero","options":["combiner","reduce","mapper","intermediate"],"correctAns":2,"difficulty":null,"optionsAsCode":false},{"query":"MapReduce applications use which of these classes to report their statistics?","options":["mapper","reducer","combiner","counter"],"correctAns":4,"difficulty":null,"optionsAsCode":false},{"query":"\\_ is the query language, and \\_ is storage for NoSQL on Hadoop","options":["HDFS; HQL","HQL; HBase","HDFS; SQL","SQL; HBase"],"correctAns":2,"difficulty":null,"optionsAsCode":false},{"query":"MapReduce 1.0 \\_ YARN","options":["does not include","is the same thing as","includes","replaces"],"correctAns":1,"difficulty":null,"optionsAsCode":false},{"query":"Which type of Hadoop node executes file system namespace operations like opening, closing, and renaming files and directories?","options":["ControllerNode","DataNode","MetadataNode","NameNode"],"correctAns":4,"difficulty":null,"optionsAsCode":false},{"query":"47 Suppose you are trying to finish a Pig script that converts text in the input string to uppercase. What code is needed on line 2 below?","options":["as (text:CHAR[]); upper_case = FOREACH data GENERATE org.apache.pig.piggybank.evaluation.string.UPPER(TEXT);","as (text:CHARARRAY); upper_case = FOREACH data GENERATE org.apache.pig.piggybank.evaluation.string.UPPER(TEXT);","as (text:CHAR[]); upper_case = FOREACH data org.apache.pig.piggybank.evaluation.string.UPPER(TEXT);","as (text:CHARARRAY); upper_case = FOREACH data org.apache.pig.piggybank.evaluation.string.UPPER(TEXT);"],"correctAns":2,"difficulty":null,"optionsAsCode":false},{"query":"In a MapReduce job, which phase runs after the Map phase completes?","options":["Combiner","Reducer","Map2","Shuffle and Sort"],"correctAns":1,"difficulty":null,"optionsAsCode":false},{"query":"Where would you configure the size of a block in a Hadoop environment?","options":["dfs.block.size in hdfs-site.xmls","orc.write.variable.length.blocks in hive-default.xml","mapreduce.job.ubertask.maxbytes in mapred-site.xml","hdfs.block.size in hdfs-site.xml"],"correctAns":1,"difficulty":null,"optionsAsCode":false},{"query":"Hadoop systems are **\\_** RDBMS systems.","options":["replacements for","not used with","substitutes for","additions for"],"correctAns":4,"difficulty":null,"optionsAsCode":false},{"query":"Which object can be used to distribute jars or libraries for use in MapReduce tasks?","options":["distributed cache","library manager","lookup store","registry"],"correctAns":1,"difficulty":null,"optionsAsCode":false},{"query":"To view the execution details of an Impala query plan, which function would you use ?","options":["explain","query action","detail","query plan"],"correctAns":1,"difficulty":null,"optionsAsCode":false},{"query":"Which feature is used to roll back a corrupted HDFS instance to a previously known good point in time?","options":["partitioning","snapshot [","replication","high availability"],"correctAns":2,"difficulty":null,"optionsAsCode":false},{"query":"Hadoop Common is written in which language?","options":["C++","C","Haskell","Java"],"correctAns":4,"difficulty":null,"optionsAsCode":false},{"query":"Which file system does Hadoop use for storage?","options":["NAS","FAT","HDFS","NFS"],"correctAns":3,"difficulty":null,"optionsAsCode":false},{"query":"What kind of storage and processing does Hadoop support?","options":["encrypted","verified","distributed","remote"],"correctAns":3,"difficulty":null,"optionsAsCode":false}]